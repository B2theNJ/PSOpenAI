{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to count tokens with PSOpenAI\n",
    "\n",
    "PSOpenAI provides `ConvertTo-Token` and `ConvertFrom-Token` commands for tokenize.\n",
    "\n",
    "Given a text string (e.g., \"PowerShell for every system!\") and an encoding (e.g., \"cl100k_base\"), a tokenizer can split the text string into a list of tokens (e.g., (\"Power\", \"Shell\", \" for\", \" every\", \" system\", \"!\")).\n",
    "\n",
    "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings\n",
    "\n",
    "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
    "\n",
    "PSOpenAI supports various encodings used by OpenAI models.\n",
    "\n",
    "|Encoding name|OpenAI models|\n",
    "|:----|:----|\n",
    "|`cl100k_base`|`gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`|\n",
    "|`p50k_base`|Codex models, `text-davinci-002`, `text-davinci-003`|\n",
    "|`p50k_edit`|`text-davinci-edit-001`|\n",
    "|`r50k_base` (or `gpt2`)|GPT-3 models like `davinci`|\n",
    "\n",
    "You can specify encoding by an encoding name or model name:\n",
    "\n",
    "```PowerShell\n",
    "ConvertTo-Token -Encoding cl100k_base\n",
    "ConvertTo-Token -Model gpt-4\n",
    "```\n",
    "\n",
    "> Note: If you don't specify any encoding or model, it will use `cl100k_base` encoding.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import\n",
    "\n",
    "Tokenizer is run on a local machine. No Internet connection or API key is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "Import-Module ..\\PSOpenAI.psd1 -Force"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Turn text into tokens with `ConvertTo-Token`\n",
    "\n",
    "The `ConvertTo-Token` converts a text string into a list of token integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15335\n",
      "26354\n",
      "369\n",
      "1475\n",
      "1887\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ConvertTo-Token -Text \"PowerShell for every system!\" -Encoding \"cl100k_base\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConvertTo-Token` also accepts input from pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15335\n",
      "26354\n",
      "369\n",
      "1475\n",
      "1887\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\"PowerShell for every system!\" | ConvertTo-Token -Encoding \"cl100k_base\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count tokens by counting the length of the list returned by `ConvertTo-Token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\r\n"
     ]
    }
   ],
   "source": [
    "$tokens = \"PowerShell for every system!\" | ConvertTo-Token -Encoding \"cl100k_base\"\n",
    "$tokens.Length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Turn tokens into text with `ConvertFrom-Token`\n",
    "\n",
    "`ConvertFrom-Token` converts a list of token integers to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerShell for every system!\r\n"
     ]
    }
   ],
   "source": [
    "ConvertFrom-Token -Token (15335, 26354, 369, 1475, 1887, 0) -Encoding \"cl100k_base\"\n",
    "\n",
    "# Also, you can input from pipeline as well\n",
    "# (15335, 26354, 369, 1475, 1887, 0) | ConvertFrom-Token -Encoding \"cl100k_base\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-AsArray` switch can also be used to convert each token into a string array. This gives you the ability to see how the text is splitted by Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power\n",
      "Shell\n",
      " for\n",
      " every\n",
      " system\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "ConvertFrom-Token -Token (15335, 26354, 369, 1475, 1887, 0) -Encoding \"cl100k_base\" -AsArray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing encodings\n",
    "\n",
    "Different encodings vary in how they split words, group spaces, and handle non-English characters. you can compare different encodings on a few example strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [],
   "source": [
    "function Compare-Encodings {\n",
    "  $text = $args[0]\n",
    "  echo ('Example string: \"{0}\"' -f $text)\n",
    "  $encs = 'gpt2', 'p50k_base', 'cl100k_base'\n",
    "\n",
    "  $encs | % {\n",
    "    # Encoding\n",
    "    $tokens = ($text | ConvertTo-Token -Encoding $_)\n",
    "    # Decoding\n",
    "    $words = ($tokens | ConvertFrom-Token -Encoding $_ -AsArray)\n",
    "    # Display\n",
    "    [pscustomobject]@{\n",
    "        \"Encoding\"      = $_\n",
    "        \"Count\"         = $tokens.Length\n",
    "        \"Token(int)\"    = $tokens -join ', '\n",
    "        \"Token(string)\" = $words -join ', '\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example string: \"antidisestablishmentarianism\"\n",
      "\n",
      "\u001b[32;1mEncoding    Count Token(int)                         Token(string)\u001b[0m\n",
      "\u001b[32;1m--------    ----- ----------                         -------------\u001b[0m\n",
      "gpt2            5 415, 29207, 44390, 3699, 1042      ant, idis, establishment, arian, ism\n",
      "p50k_base       5 415, 29207, 44390, 3699, 1042      ant, idis, establishment, arian, ism\n",
      "cl100k_base     6 519, 85342, 34500, 479, 8997, 2191 ant, idis, establish, ment, arian, ism\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Compare-Encodings \"antidisestablishmentarianism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example string: \"2 + 2 = 4\"\n",
      "\n",
      "\u001b[32;1mEncoding    Count Token(int)                     Token(string)\u001b[0m\n",
      "\u001b[32;1m--------    ----- ----------                     -------------\u001b[0m\n",
      "gpt2            5 17, 1343, 362, 796, 604        2,  +,  2,  =,  4\n",
      "p50k_base       5 17, 1343, 362, 796, 604        2,  +,  2,  =,  4\n",
      "cl100k_base     7 17, 489, 220, 17, 284, 220, 19 2,  +,  , 2,  =,  , 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Compare-Encodings \"2 + 2 = 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example string: \"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜüéâ\"\n",
      "\n",
      "\u001b[32;1mEncoding    Count Token(int)\u001b[0m\n",
      "\u001b[32;1m--------    ----- ----------                                                                       \u001b[0m\n",
      "gpt2           17 2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 295‚Ä¶\n",
      "p50k_base      17 2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 295‚Ä¶\n",
      "cl100k_base    12 33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699, 9468, 236, 231       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Compare-Encodings \"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜüéâ\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Counting tokens for chat API calls\n",
    "\n",
    "ChatGPT models like `gpt-3.5-turbo` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
    "\n",
    "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo-0301` or `gpt-4-0314`.\n",
    "\n",
    "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [],
   "source": [
    "function Measure-TokensFromMessages ($Messages, $Model) {\n",
    "  # Returns the number of tokens used by a list of messages.\n",
    "  # Note: this function is ported from openai-cookbook.\n",
    "  switch -wildcard  ($Model) {\n",
    "    'gpt-3.5-turbo*' { \n",
    "      $tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "      $tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    }\n",
    "    'gpt-4*' { \n",
    "      $tokens_per_message = 3\n",
    "      $tokens_per_name = 1\n",
    "    }\n",
    "    Default {\n",
    "      Write-Error \"Not implemented for model $Model.\"\n",
    "      return\n",
    "    }\n",
    "  }\n",
    "  $num_tokens = 0\n",
    "  foreach ($message in $Messages) {\n",
    "    $num_tokens += $tokens_per_message\n",
    "    foreach ($item in $message.GetEnumerator()) {\n",
    "      $num_tokens += @(ConvertTo-Token -Text $item.Value -Model $Model).Count\n",
    "      if ($item.Key -eq 'name') {\n",
    "        $num_tokens += $tokens_per_name\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  $num_tokens += 3 # every reply is primed with <|start|>assistant<|message|>\n",
    "  Write-Output $num_tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\r\n",
      "127 prompt tokens counted by Measure-TokensFromMessages function.\r\n",
      "127 prompt tokens counted by the OpenAI API.\r\n",
      "\r\n",
      "gpt-4-0314\r\n",
      "129 prompt tokens counted by Measure-TokensFromMessages function.\r\n",
      "129 prompt tokens counted by the OpenAI API.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "$ExampleMessages = [pscustomobject]@{\n",
    "  History = @(\n",
    "    @{\n",
    "      'role'    = 'system'\n",
    "      'content' = 'You are a helpful, pattern-following assistant that translates corporate jargon into plain English.'\n",
    "    },\n",
    "    @{\n",
    "      'role'    = 'system'\n",
    "      'name'    = 'example_user'\n",
    "      'content' = 'New synergies will help drive top-line growth.'\n",
    "    },\n",
    "    @{\n",
    "      'role'    = 'system'\n",
    "      'name'    = 'example_assistant'\n",
    "      'content' = 'Things working well together will increase revenue.'\n",
    "    },\n",
    "    @{\n",
    "      'role'    = 'system'\n",
    "      'name'    = 'example_user'\n",
    "      'content' = \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"\n",
    "    },\n",
    "    @{\n",
    "      'role'    = 'system'\n",
    "      'name'    = 'example_assistant'\n",
    "      'content' = \"Let's talk later when we're less busy about how to do better.\"\n",
    "    }\n",
    "  )\n",
    "  Message = @{\n",
    "    'role'    = 'user'\n",
    "    'content' = \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "'gpt-3.5-turbo-0301', 'gpt-4-0314' | ForEach-Object {\n",
    "  $model = $_\n",
    "  echo $model\n",
    "  # example token count from the function defined above\n",
    "  \"{0} prompt tokens counted by Measure-TokensFromMessages function.\" -f (Measure-TokensFromMessages -Messages ($ExampleMessages.History + $ExampleMessages.Message) -Model $model)\n",
    "  # example token count from the OpenAI API\n",
    "  $response = $ExampleMessages | Request-ChatGPT `\n",
    "    -Model $model `\n",
    "    -Message $ExampleMessages.Message.content `\n",
    "    -Temperature 0 `\n",
    "    -MaxTokens 1  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "  \"{0} prompt tokens counted by the OpenAI API.`r`n\" -f $response.usage.prompt_tokens\n",
    "} | Out-String"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "pwsh",
      "name": "pwsh"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
