---
external help file: PSOpenAI-help.xml
Module Name: PSOpenAI
online version: https://github.com/mkht/PSOpenAI/blob/main/Docs/Request-AzureTextCompletion.md
schema: 2.0.0
---

# Request-AzureTextCompletion

## SYNOPSIS
Creates a completion for the provided prompt and parameters.

## SYNTAX

```
Request-AzureTextCompletion
    [[-Prompt] <String[]>]
    [-Suffix <String>]
    -Deployment <String>
    [-Temperature <Double>]
    [-TopP <Double>]
    [-NumberOfAnswers <UInt16>]
    [-Stream]
    [-StopSequence <String[]>]
    [-MaxTokens <Int32>]
    [-PresencePenalty <Double>]
    [-FrequencyPenalty <Double>]
    [-LogitBias <IDictionary>]
    [-User <String>]
    [-Echo <Boolean>]
    [-BestOf <UInt16>]
    [-TimeoutSec <Int32>]
    [-MaxRetryCount <Int32>]
    [-ApiBase <System.Uri>]
    [-ApiVersion <string>]
    [-ApiKey <Object>]
    [-AuthType <string>]
    [<CommonParameters>]
```

## DESCRIPTION
Given a prompt, the AI model will return one or more predicted completions.  
https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#completions

## EXAMPLES
### Example 1: Estimate the sentences that follow.
```PowerShell
PS C:\> $global:OPENAI_API_KEY = '<Put your api key here>'
PS C:\> $global:OPENAI_API_BASE  = 'https://<resource-name>.openai.azure.com/'
PS C:\> Request-TextCompletion -Prompt 'This is a hamburger store.' -Deployment 'YourDeploymentName' | select Answer
```
```
We serves
-classic hamburgers
-tofu burgers
```

## PARAMETERS

### -Prompt
(Required)
The prompt(s) to generate completions for

```yaml
Type: String[]
Aliases: Message
Required: False
Position: 1
Accept pipeline input: True (ByValue)
```

### -Suffix
The suffix that comes after a completion of inserted text.

```yaml
Type: String
Required: False
Position: Named
```

### -Deployment
The deployment name you chose when you deployed the model.  
Deployments must be created in Azure Portal in advance.

```yaml
Type: String
Required: True
Position: Named
```

### -Temperature
What sampling temperature to use, between `0` and `2`.  
Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic.

```yaml
Type: Double
Required: False
Position: Named
```

### -TopP
An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.  
So `0.1` means only the tokens comprising the top `10%` probability mass are considered.

```yaml
Type: Double
Aliases: top_p
Required: False
Position: Named
```

### -NumberOfAnswers
How many texts to generate for each prompt.
The default value is `1`.

```yaml
Type: UInt16
Aliases: n
Required: False
Position: Named
Default value: 1
```

### -Stream
Whether to stream back partial progress.

```yaml
Type: System.Management.Automation.SwitchParameter
Required: False
Position: Named
Default value: False
```

### -StopSequence
Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.

```yaml
Type: String[]
Aliases: stop
Required: False
Position: Named
```

### -MaxTokens
The maximum number of tokens allowed for the generated answer.  
The max value depends on models.

```yaml
Type: Int32
Aliases: max_tokens
Required: False
Position: Named
Default value: 2048
```

### -PresencePenalty
Number between `-2.0` and `2.0`.  
Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

```yaml
Type: Double
Aliases: presence_penalty
Required: False
Position: Named
```

### -FrequencyPenalty
Number between `-2.0` and `2.0`.  
Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

```yaml
Type: Double
Aliases: frequency_penalty
Required: False
Position: Named
```

### -LogitBias
Modify the likelihood of specified tokens appearing in the completion.  
Accepts a maps of tokens to an associated bias value from `-100` to `100`. You can use `ConvertTo-Token` to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between `-1` and `1` should decrease or increase likelihood of selection; values like `-100` or `100` should result in a ban or exclusive selection of the relevant token.  
As an example, you can pass like so: `@{23182 = 20; 88847 = -100}`  
ID 23182 maps to "apple" and ID 88847 maps to "banana". Thus, this example increases the likelihood of the word "apple" being included in the response from the AI and greatly reduces the likelihood of the word "banana" being included.

```yaml
Type: IDictionary
Aliases: logit_bias
Required: False
Position: Named
```

### -User
A unique identifier representing for your end-user. This will help Azure OpenAI monitor and detect abuse.

```yaml
Type: String
Required: False
Position: Named
```

### -Echo
Echo back the prompt in addition to the completion.
The default value is `$false`.

```yaml
Type: Boolean
Required: False
Position: Named
Default value: $false
```

### -BestOf
Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token).

```yaml
Type: UInt16
Aliases: best_of
Required: False
Position: Named
```

### -TimeoutSec
Specifies how long the request can be pending before it times out.
The default value is `0` (infinite).

```yaml
Type: Int32
Required: False
Position: Named
Default value: 0
```

### -MaxRetryCount
Number between `0` and `100`.  
Specifies the maximum number of retries if the request fails.  
The default value is `0` (No retry).  
Note 1: Retries will only be performed if the request fails with a `429 (Rate limit reached)` or `5xx (Server side errors)` error. Other errors (e.g., authentication failure) will not be performed.  
Note 2: Retry intervals increase exponentially with jitters, such as `1s > 2s > 4s > 8s > 16s`

```yaml
Type: Int32
Required: False
Position: Named
Default value: 0
```

### -ApiBase
Specifies yhe name of your Azure OpenAI resource endpoint such like: 
`https://{your-resource-name}.openai.azure.com/`  
If not specified, it will try to use `$global:OPENAI_API_BASE` or `$env:OPENAI_API_BASE`

```yaml
Type: System.Uri
Required: False
Position: Named
```

### -ApiVersion
The API version to use for this operation.  
The default value is `2023-03-15-preview`

```yaml
Type: string
Required: False
Position: Named
Default value: "2023-03-15-preview"
```

### -ApiKey
Specifies API key for authentication.  
The type of data should `[string]` or `[securestring]`.  
If not specified, it will try to use `$global:OPENAI_API_KEY` or `$env:OPENAI_API_KEY`

```yaml
Type: Object
Required: False
Position: Named
```

### -AuthType
Specifies the authentication type.  
You can choose from `azure` or `azure_ad`.  
The default value is `azure`

```yaml
Type: string
Required: False
Position: Named
Default value: "azure"
```


## INPUTS

## OUTPUTS

### [pscustomobject]
## NOTES

## RELATED LINKS

[https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#completions](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#completions)
